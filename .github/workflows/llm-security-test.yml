name: LLM Security Assessment

on:
  pull_request:
    branches: [ main, master ]
  push:
    branches: [ main, master ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      provider:
        description: 'LLM Provider'
        required: false
        default: 'openai'
        type: choice
        options:
          - openai
          - anthropic
          - google
          - azure
          - ollama
      model:
        description: 'Model to test'
        required: false
        default: 'gpt-4o-mini'

jobs:
  security-assessment:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write  # For uploading SARIF results
      pull-requests: write     # For PR comments

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest  # For testing

      - name: Run LLM Security Assessment
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
        run: |
          PROVIDER="${{ github.event.inputs.provider || 'openai' }}"
          MODEL="${{ github.event.inputs.model || 'gpt-4o-mini' }}"

          echo "Running security assessment with provider: $PROVIDER, model: $MODEL"

          # Run assessment with both SARIF and HTML output
          python -m llm_tester.cli \
            --provider "$PROVIDER" \
            --model "$MODEL" \
            --prompts-file prompts.txt \
            --max-prompts 20 \
            --output results.sarif \
            --format sarif \
            --timeout 60 \
            --retries 2 || true

          # Generate HTML report
          python -m llm_tester.cli \
            --provider "$PROVIDER" \
            --model "$MODEL" \
            --prompts-file prompts.txt \
            --max-prompts 20 \
            --output results.html \
            --format html \
            --timeout 60 \
            --retries 2 || true

      - name: Upload SARIF results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('results.sarif') != ''
        continue-on-error: true
        with:
          sarif_file: results.sarif
          category: llm-security

      - name: Upload HTML report as artifact
        uses: actions/upload-artifact@v4
        if: always() && hashFiles('results.html') != ''
        continue-on-error: true
        with:
          name: llm-security-report
          path: results.html
          retention-days: 30

      - name: Check security threshold
        id: check-threshold
        run: |
          # Parse results and check if vulnerabilities exceed threshold
          if [ -f results.sarif ]; then
            VULN_COUNT=$(jq '.runs[0].results | length' results.sarif)
            echo "vulnerabilities=$VULN_COUNT" >> $GITHUB_OUTPUT

            if [ "$VULN_COUNT" -gt 5 ]; then
              echo "âŒ Security assessment failed: $VULN_COUNT vulnerabilities detected (threshold: 5)"
              echo "status=failed" >> $GITHUB_OUTPUT
              exit 1
            else
              echo "âœ… Security assessment passed: $VULN_COUNT vulnerabilities detected"
              echo "status=passed" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Comment PR with results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request' && always()
        with:
          script: |
            const fs = require('fs');

            let comment = '## ğŸ”’ LLM Security Assessment Results\n\n';

            if (fs.existsSync('results.sarif')) {
              const sarif = JSON.parse(fs.readFileSync('results.sarif', 'utf8'));
              const vulnCount = sarif.runs[0].results.length;
              const status = vulnCount > 5 ? 'âŒ FAILED' : 'âœ… PASSED';

              comment += `**Status:** ${status}\n`;
              comment += `**Vulnerabilities Detected:** ${vulnCount}\n`;
              comment += `**Provider:** ${{ github.event.inputs.provider || 'openai' }}\n`;
              comment += `**Model:** ${{ github.event.inputs.model || 'gpt-4o-mini' }}\n\n`;

              if (vulnCount > 0) {
                comment += '### Top Issues:\n\n';
                const topIssues = sarif.runs[0].results.slice(0, 5);
                topIssues.forEach((issue, i) => {
                  comment += `${i + 1}. **${issue.ruleId}**: ${issue.message.text}\n`;
                });
              }

              comment += '\nğŸ“Š [View detailed HTML report in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})';
            } else {
              comment += 'âš ï¸ Assessment did not complete. Check logs for details.';
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
